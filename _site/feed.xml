<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nikhil Buduma</title>
    <description>Nikhil Buduma&#39;s Blog</description>
    <link>http://nikhilbuduma.com</link>
    <atom:link href="http://nikhilbuduma.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>A Deep Dive into Recurrent Neural Nets</title>
        <description>&lt;p&gt;Last time, we talked about the traditional feed-forward neural net and concepts that form the basis of deep learning. These ideas are extremely powerful! We saw how feed-forward convolutional neural networks have set records on many difficult tasks including handwritten digit recognition and object classification. And even today, feed-forward neural networks consistently outperform virtually all other approaches to solving classification tasks.&lt;/p&gt;

&lt;p&gt;And yet, despite their well celebrated successes, most experts would agree that feed-forward neural nets are still rather limited in what they can achieve. Why? Because the task of &quot;classification&quot; is only one small component of the incredible computational power of the human brain. We&#39;re wired not only to recognize individual instances but to also analyze entire sequences of inputs. These sequences are ultra rich in information, have complex time dependencies, and can be of arbitrary length. For example, vision, motor control, speech, and comprehension all require us to process high-dimensional inputs as they change over time. This is something that feed-forward nets are incredibly poor at modeling.&lt;/p&gt;

&lt;h3&gt;What is a Recurrent Neural Net?&lt;/h3&gt;

&lt;p&gt;One quite promising solution to tackling the problem of learning sequences of information is the recurrent neural network (RNN). RNNs are built on the same computational unit as the feed forward neural net, but differ in the architecture of how these neurons are connected to one another. Feed forward neural networks were organized in layers, where information flowed unidirectionally from input units to output units. There were no undirected cycles in the connectivity patterns. Although neurons in the brain do contain undirected cycles as well as connections within layers, we chose to impose these restrictions to simplify the training process at the expense of computational versatility. Thus, to create more powerful compuational systems, we allow RNNs to break these artificially imposed rules. RNNs do not have to be organized in layers and directed cycles are allowed. In fact, neurons are actually allowed to be connected to themselves.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/rnn_ex.png&quot; title=&quot;Example Recurrent Neural Net&quot; alt=&quot;Example Recurrent Neural Net&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;An example schematic of a RNN with directed cycles and self connectivities&lt;/h6&gt;

&lt;p&gt;The RNN consists of a bunch of input units, labeled &lt;script type=&quot;math/tex&quot;&gt;u_1,...,u_K&lt;/script&gt; and output units, labeled &lt;script type=&quot;math/tex&quot;&gt;y_1,...,y_L&lt;/script&gt;. There are also the hidden units &lt;script type=&quot;math/tex&quot;&gt;x_1,...x_N&lt;/script&gt;, which do most of the interesting work. You&#39;ll notice that the illustration shows a unidirectional flow of information from the input units to the hidden units as well as another unidirectional flow of information from the hidden units to the output units. In some cases, RNNs break the latter restriction with connections leading from the output units back to the hidden units. These are called &quot;backprojections,&quot; and don&#39;t make the analysis of RNNs too much more complicated. The same techniques we will discuss here will also apply to RNNs with backprojections.&lt;/p&gt;

&lt;p&gt;There are a lot of pretty challenging technical difficulties that arise when training recurrent neural networks, and it&#39;s still a very active area of research. Hopefully by the end of this article, we&#39;ll have a solid understanding of how RNNs work and some of the results that have been achieved!&lt;/p&gt;

&lt;h3&gt;Simulating a Recurrent Neural Network&lt;/h3&gt;

&lt;p&gt;Now that we understand how a RNN is structured, we can discuss how it&#39;s able to simulate a sequence of events. Let&#39;s consider a neat toy example of a recurrent neural net acting like an timer module, a classic example designed by Herbert Jaeger (his original manuscript can be found &lt;a target=&#39;_blank&#39; href=&#39;http://www.pdx.edu/sites/www.pdx.edu.sysc/files/Jaeger_TrainingRNNsTutorial.2005.pdf&#39;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/timer_ex.png&quot; title=&quot;Timer RNN input/output&quot; alt=&quot;Timer RNN input/output&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;A simple example of how a perfect RNN would simulate a timer&lt;/h6&gt;

&lt;p&gt;In this case, we have two inputs. The input &lt;script type=&quot;math/tex&quot;&gt;u_1&lt;/script&gt; corresponds to a binary switch which spikes to one when the RNN is supposed to start the timer. The input &lt;script type=&quot;math/tex&quot;&gt;u_2&lt;/script&gt; is a discrete variable that varies between 0.1 and 1.0 inclusive which corresponds to how long the output should be turned on if the timer is started at that instant. The RNN&#39;s specification requires it to turn on the output for a duration of &lt;script type=&quot;math/tex&quot;&gt;1000u_2&lt;/script&gt;. Finally, the outputs in the training examples toggle between 0 (off) and 0.5 (on).&lt;/p&gt;

&lt;p&gt;But how exactly would a neural net achieve this calculation? First, the RNN has all of its hidden activities initialized to some pre-determined state. Then at each time step (time &lt;script type=&quot;math/tex&quot;&gt;t = 1,2,...&lt;/script&gt;), every hidden unit sends its current activity through all its outgoing connections. It then recalculate its new activity by computing the weighted sum (logit) of its inputs from other neurons (including itself if there is a self connection) and the current values of the inputs, and then feeding this value into a neuron-specific function (a straightforward copy operation, sigmoid transform, soft-max, etc.). Because the previous vector of activities is used to compute the vector of activies in each time step, RNNs are able to retain memory of previous events and utlize this memory in making decisions.&lt;/p&gt;

&lt;p&gt;Clearly a neural net would be unlikely to perfectly perform according to specification, but you can imagine it outputting a result (orange) that looks pretty darn close to the ground truth (blue) after training the RNN with hundreds or thousands of examples. We&#39;ll talk more about approaches to training RNNs in the following sections.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/timer_fit.png&quot; title=&quot;Timer RNN Output&quot; alt=&quot;Timer RNN outupt&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;An example fit for how a well-trained RNN might approximate the output of a test case&lt;/h6&gt;

&lt;p&gt;At this point, you&#39;re probably thinking that this is pretty cool, but it&#39;s still a pretty contrived example. What&#39;s the strategy for using RNNs in practice? We examine real systems and their behaviors over time in response to stimuli. For example, you might teach a RNN to transcribe audio into text by building a dataset (in a sense, observing the response of the human auditory system in response to the inputs in the training set). You may also use a trained neural net to model a system&#39;s reactions under novel stimuli.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/trifecta.png&quot; title=&quot;Trifecta&quot; alt=&quot;Trifecta&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;How a RNN might be used in practice&lt;/h6&gt;

&lt;p&gt;But if you&#39;re creative, you can use RNNs in ways that seem pretty spectacular. For example, a specialized kind of RNN, called a long short-term RNN or LSTM, has been used to achieve spectacular rates of data compression (although the current approach to RNN-based compression does take a significant amount time). For those itching to learn more, we&#39;ll talk about the LSTM architecture in a later section.&lt;/p&gt;

&lt;h3&gt;Training a RNN - Backpropagation Through Time&lt;/h3&gt;

&lt;p&gt;Great, now we understand what a RNN is and how it works, but how do we train a RNN in the first place to achieve all of these spectacular feats? Specifically, how do we determine the weights that are on each of the connections? And how do we choose the initial activities of all of the hidden units? Our first instinct might be to use backpropagation directly, after all it worked quite well when we used it on feed forward neural nets.&lt;/p&gt;

&lt;p&gt;The problem with using backpropagation here is that we have cyclical dependencies. In feed forward nets, when we calculated the error derivatives with respect to the weights in one layer, we could express them completely in terms of the error derivatives from the layer above. In a recurrent neural network, we don&#39;t have this nice layering because the neurons do not form a directed acyclic graph. Trying to backpropagate through a RNN could force us to try to express an error derivative in terms of itself, which doesn&#39;t make for easy analysis.&lt;/p&gt;

&lt;p&gt;So how can we use backpropagation for RNNs, if at all? The answer lies in employing a clever transformation, where we convert our RNN into a new structure that&#39;s essentially a feed-forward neural network! We call this strategy &quot;unrolling&quot; the RNN through time, and an example can be seen in the figure below (with only one input/output per time step to simplify the illustration):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/rnn_unroll.png&quot; title=&quot;RNN unrolled&quot; alt=&quot;RNN unrolled&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;An example of &quot;unrolling&quot; and RNN through time to use backpropagation&lt;/h6&gt;

&lt;p&gt;The process is actually quite simple, but it has a profound impact on our ability to analyze the neural network. We take the RNN&#39;s inputs, outputs, and hidden units and replicate it for every time step. These replications correspond to layers in our new feed forward neural network. We then connect hidden units as follows. If the original RNN has a connection of weight &lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt; from neuron &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; to neuron &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, in our feed forward neural net, we draw a connection of weight &lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt; from neuron &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; in every layer &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; to neuron &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in every layer &lt;script type=&quot;math/tex&quot;&gt;t_{k+1}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Thus, to train our RNN, we randomly initialize the weights, &quot;unroll&quot; it into a feed forward neural net, and backpropogate to determine the optimal weights! To determine the initializations for the hidden states at time 0, we can treat the initial activities as parameters fed into the feed forward network at the lowest layer and backpropagate to determine their optimal values as well!&lt;/p&gt;

&lt;p&gt;We run into a problem however, which is that after every batch of training examples we use, we need to modify the weights based on the error derivatives we calculated. In our feed-forward net, we have sets of connections that all correspond to the same connection in the original RNN. The error derivatives calculated with respect to their weights, however, are not guaranteed to be equal, which means we might be modifying them by different amounts. We definitely don&#39;t want to be doing that!&lt;/p&gt;

&lt;p&gt;We can get around this challenge, by averaging (or summing) the error derivatives over all the connections that belong to the same set. This means that after each batch, we modify corresponding connections by the same amount, so if they were initialized to the same value, they will end up at the same value. This solves our problem :)&lt;/p&gt;

&lt;h3&gt;The Problems with Deep Backpropagation&lt;/h3&gt;

&lt;p&gt;Unlike traditional feed forward nets, the feed forward nets generated by unrolling RNNs can be enormously deep. This gives rise to a serious practical issue: it can be obscenely difficult to train using the backpropagation through time approach. Let&#39;s take a step back and try to understand why.&lt;/p&gt;

&lt;p&gt;Let&#39;s try to train a RNN to do a very primitive task. Let&#39;s give the RNN a single hidden unit with a bias term, and we&#39;ll connect it to itself and a singular output. We want this neural network to output a fixed target value after 50 steps, let&#39;s say 0.7. We&#39;ll use the squared error of the output on the 50th time step as our error function, which we can plot as a surface over the value of the weight and the bias:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/rnn_error_surface.png&quot; title=&quot;Error Surface&quot; alt=&quot;Error Surface&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;The problematic error surface of a simple RNN (image from: &lt;a target=&#39;_blank&#39; href=&#39;http://arxiv.org/pdf/1211.5063v2.pdf&#39;&gt;Pascanu et al.&lt;/a&gt;)&lt;/h6&gt;

&lt;p&gt;Now, let&#39;s say we started at the red star (using a random initialization of weights). You&#39;ll notice that as we use gradient descent, we get closer and closer to the local minimum on the surface. But suddenly, when we slightly overreach the valley and hit the cliff, we are presented with a massive gradient in the opposite direction. This forces us to bounce extremely far away from the local minimum. And once we&#39;re in nowhere land, we quickly find that the gradients are so vanishingly small that coming close again will take a seemingly endless amount of time. This issue is called the problem of &lt;em&gt;exploding and vanishing gradients&lt;/em&gt;. You can imagine perhaps controlling this issue by rescaling gradients to never exceed a maximal magnitude (see the dotted path after hitting the cliff), but this approach still doesn&#39;t perform spectacularly well, especially in more complex RNNs. For a more mathematical treatment of this issue, check out this &lt;a target=&#39;_blank&#39; href=&#39;http://arxiv.org/pdf/1211.5063v2.pdf&#39;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Long Short Term Memory&lt;/h3&gt;

&lt;p&gt;To address these problems, researchers proposed a modified architecture for recurrent neural networks to help bridge long time lags between forcing inputs and appropriate responses and protect against exploding gradients. The architecture forces constant error flow (thus, neither exploding nor vanishing) through the internal state of special memory units. This long short term memory (LSTM) architecture utlized units that were structured as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lstm.png&quot; title=&quot;LSTM&quot; alt=&quot;LSTM&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Structure of the basic LSTM unit&lt;/h6&gt;

&lt;p&gt;The LSTM unit consists of a memory cell which attempts to store information for extended periods of time. Access to this memory cell is protected by specialized gate neurons - the keep, write, and read gates - which are all logistic units. These gate cells, instead of sending their activities as inputs to other neurons, set the weights on edges connecting the rest of the neural net to the memory cell. The memory cell is a linear neuron that has a connection to itself. When the keep gate is turned on (with an activity of 1), the self connection has weight one and the memory cell writes its contents into itself. When the keep gate outputs a zero, the memory cell forgets its previous contents. The write gate allows the rest of the neural net to write into the memory cell when it outputs a 1 while the read gate allows the rest of the neural net to read from the memory cell when it outputs a 1.&lt;/p&gt;

&lt;p&gt;So how exactly does this force a constant error flow through time to locally protect against exploding and vanishing gradients? To visualize this, let&#39;s unroll the LSTM unit through time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/lstm_unroll.png&quot; title=&quot;LSTM unrolled&quot; alt=&quot;LSTM unrolled&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Unrolling the LSTM unit through the time domain&lt;/h6&gt;

&lt;p&gt;At first, the keep gate is set to 0 and the write gate is set to 1, which places 4.2 into the memory cell. This value is retained in the memory cell by a subsequent keep value of 1 and protected from read/write by values of 0. Finally, the cell is read and then cleared. Now we try to follow the backpropagation from the point of loading 4.2 into the memory cell to the point of reading 4.2 from the cell and its subsequent clearing. We realize that due to the linear nature of the memory neuron, the error derivative that we receive from the read point backpropagates with negligible change until the write point because the weights of the connections connecting the memory cell through all the time layers have weights approximately equal to 1 (approximate because of the logistic output of the keep gate). As a result, we can locally preserve the error derivatives over hundreds of steps without having to worry about exploding or vanishing gradients. You can see the action of this method successfully reading cursive handwriting:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;br/&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/mLxsbWAYIpw&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br/&gt;
&lt;/div&gt;


&lt;p&gt;The animation, borrowed from neural networks expert Alex Graves, requires a little bit of explanation:&lt;/p&gt;

&lt;p&gt;1) Row 1: Shows when the letters are recognized&lt;/p&gt;

&lt;p&gt;2) Row 2: Shows the states of some of the memory cells (Notice how they get reset when a character is recognized!)&lt;/p&gt;

&lt;p&gt;3) Row 3: Shows the writing as it&#39;s being analyzed by the LSTM RNN&lt;/p&gt;

&lt;p&gt;4) Row 4: This shows the gradient backpropagated to the inputs from the most active character of the upper soft-max layer (This tells you which data points are providing the most influence on your current decision for the character)&lt;/p&gt;

&lt;p&gt;The LSTM RNN does quite well, and it&#39;s been applied in lots of other places as well. As we discussed earlier, deep architectures for LSTM RNNs have been used to achieve pretty astonishing data compression rates. For those who are interested in learning more about this particular application for LSTM RNNs can check out this &lt;a target=&#39;_blank&#39; href=&#39;http://arxiv.org/pdf/1308.0850.pdf&#39;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;How to effectively train neural nets remains an area of active research and has resulted in a number of alternative approaches, with no clear winner. The LSTM RNN architecture is one such approach to improving the training of RNNs. Another approach is to use a much better optimzer that can deal with exploding and vanishing gradients. Hessian-free optimization tries to detect directions with a small gradient, but even smaller curvature. This problem allows it to perform much better than naive gradient descent. A third approach involves a very careful initialization of the weights in hopes that it will allow us to avoid the problem of exploding and vanishing gradients in the first place (e.g. echo state networks, momentum based approaches).&lt;/p&gt;

&lt;p&gt;RNNs are pretty powerful stuff, and I&#39;m quite excited to see what comes out of this area of active research over the next few years. If any of these topics that I briefly touched upon in the previous paragraph seem interesting, shoot me an email at nkbuduma@gmail.com! I can write another blog post exploring other approaches to RNNs. Thanks for all of the support and I hope you enjoy this article ❤&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Jan 2015 00:00:00 -0800</pubDate>
        <link>http://nikhilbuduma.com//2015/01/11/a-deep-dive-into-recurrent-neural-networks/</link>
        <guid isPermaLink="true">http://nikhilbuduma.com//2015/01/11/a-deep-dive-into-recurrent-neural-networks/</guid>
      </item>
    
      <item>
        <title>Deep Learning in a Nutshell</title>
        <description>&lt;p&gt;Deep learning. Neural networks. Backpropagation. Over the past year or two, I&#39;ve heard these buzz words being tossed around a lot, and it&#39;s something that has definitely seized my curiosity recently. Deep learning is an area of active research these days, and if you&#39;ve kept up with the field of computer science, I&#39;m sure you&#39;ve come across at least some of these terms at least once.&lt;/p&gt;

&lt;p&gt;Deep learning can be an indimidating concept, but it&#39;s becoming increasingly important these days. Google&#39;s already making huge strides in the space with the &lt;a target=&#39;_blank&#39; href=&#39;http://www.wired.com/2014/07/google_brain/&#39;&gt;Google Brain project&lt;/a&gt; and its recent acquisition of the London-based deep learning startup &lt;a target=&#39;blank&#39; href=&#39;http://deepmind.com/&#39;&gt;DeepMind&lt;/a&gt;. Moreover, deep learning methods are beating out traditional machine learning approaches on virtually every single metric.&lt;/p&gt;

&lt;p&gt;So what exactly is deep learning? How does it work? And most importantly, why should you even care?&lt;/p&gt;

&lt;h3&gt;Note to the Reader&lt;/h3&gt;

&lt;p&gt;If you&#39;re new to computer science, and you&#39;ve followed me up till this point, please stick with me. Certain optional sections of this article may get a little math heavy (marked with a *), but I want to make this subject accessible to everyone, computer science major or not. In fact, if you are reading through this article and, at any point, you find yourself confused about the material, please email me. I will make whatever edits and clarifications that are necessary to make the article clearer.&lt;/p&gt;

&lt;h3&gt;What is Machine Learning?&lt;/h3&gt;

&lt;p&gt;Before we dive into deep learning, I want to take a step back and talk a little bit about the broader field of &quot;machine learning&quot; and what it means when we say that we&#39;re programming machines to &lt;em&gt;learn&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Sometimes we encounter problems for which it&#39;s really hard to write a computer program to solve. For example, let&#39;s say we wanted to program a computer to recognize hand-written digits:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/mnist.gif&quot; title=&quot;Handwritten Digits&quot; alt=&quot;Handwritten Digits&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Image Provided by the MNIST handwritten database&lt;/h6&gt;

&lt;p&gt;You could imagine trying to devise a set of rules to distinguish each individual digit. Zeros, for instance, are basically one closed loop. But what if the person didn&#39;t perfectly close the loop. Or what if the right top of the loop closes below where the left top of the loop starts?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/zero.png&quot; title=&quot;Handwritten Zero or Six&quot; alt=&quot;Handwritten Zero or Six&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;A zero that&#39;s difficult to distinguish from a six algorithmically&lt;/h6&gt;

&lt;p&gt;In this case, we have difficulty differentiating zeroes from sixes. We could establish some sort of cutoff, but how would you decide the cutoff in the first place? As you can see, it quickly becomes quite complicated to compile a list of heuristics (i.e., rules and guesses) that accurately classifies handwritten digits.&lt;/p&gt;

&lt;p&gt;And there are so many more classes of problems that fall into this category. Recognizing objects, understanding concepts, comprehending speech. We don&#39;t know what program to write because we still don&#39;t know how it&#39;s done by our own brains. And even if we did have a good idea about how to do it, the program might be horrendously complicated.&lt;/p&gt;

&lt;p&gt;So instead of trying to write a program, we try to develop an algorithm that a computer can use to look at hundreds or thousands of examples (and the correct answers), and then the computer uses that experience to solve the same problem in new situations. Essentially, our goal is to teach the computer to solve by example, very similar to how we might teach a young child to distinguish a cat from a dog.&lt;/p&gt;

&lt;p&gt;Over the past few decades, computer scientists have developed a number of algorithms that try to allow computers to learn to solve problems through examples. Deep learning, which was first theorized in the early 80&#39;s (and perhaps even earlier), is one paradigm for performing machine learning. And because of a flurry of modern research, deep learning is again on the rise because it&#39;s been shown to be quite good at teaching computers to do what our brains can do naturally.&lt;/p&gt;

&lt;p&gt;One of the big challenges with traditional machine learning models is a process called &lt;em&gt;feature extraction&lt;/em&gt;. Specifically, the programmer needs to tell the computer what kinds of things it should be looking for that will be informative in making a decision. Feeding the algorithm raw data rarely ever works, so feature extraction is a critical part of the traditional machine learning workflow. This places a huge burden on the programmer, and the algorithm&#39;s effectiveness relies heavily on how insightful the programmer is. For complex problems such as object recognition or handwriting recognition, this is a huge challenge.&lt;/p&gt;

&lt;p&gt;Deep learning is one of the only methods by which we can circumvent the challenges of feature extraction. This is because deep learning models are capable of learning to focus on the right features by themselves, requiring little guidance from the programmer. This makes deep learning an extremely powerful tool for modern machine learning.&lt;/p&gt;

&lt;h3&gt;A First Look at Neural Networks&lt;/h3&gt;

&lt;p&gt;Deep learning is a form of machine learning that uses a model of computing that&#39;s very much inspired by the structure of the brain. Hence we call this model a &lt;em&gt;neural network&lt;/em&gt;. The basic foundational unit of a neural network is the &lt;em&gt;neuron&lt;/em&gt;, which is actually conceptually quite simple.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/neuron.png&quot; title=&quot;Neuron&quot; alt=&quot;Neuron&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Schematic for a neuron in a neural net&lt;/h6&gt;

&lt;p&gt;Each neuron has a set of inputs, each of which is given a specific weight. The neuron computes some function on these weighted inputs. A linear neuron takes a linear combination of the weighted inputs. A sigmoidal neuron does something a little more complicated:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sigmoid.png&quot; title=&quot;Sigmoid&quot; alt=&quot;Sigmoid&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;The function of a sigmoidal neuron&lt;/h6&gt;

&lt;p&gt;It feeds the weighted sum of the inputs into the &lt;em&gt;logistic function&lt;/em&gt;. The logistic function returns a value between 0 and 1. When the weighted sum is very negative, the return value is very close to 0. When the weighted sum is very large and positive, the return value is very close to 1. For the more mathematically inclined, the logistic function is a good choice because it has a nice looking derivative, which makes learning a simpler process. But technical details aside, whatever function the neuron uses, the value it computes is transmitted to other neurons as its output. In practice, sigmoidal neurons are used much more often than linear neurons because they enable much more versatile learning algorithms compared to linear neurons.&lt;/p&gt;

&lt;p&gt;A neural network comes about when we start hooking up neurons to each other, to the input data, and to the &quot;outlets,&quot; which correspond to the network&#39;s answer to the learning problem. To make this structure easier to visualize, I&#39;ve included a simple example of a neural net below. We let &lt;script type=&quot;math/tex&quot;&gt;w_{i,j}^{(k)}&lt;/script&gt; be the weight of the link connecting the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; neuron in the &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; layer with the &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; neuron in the &lt;script type=&quot;math/tex&quot;&gt;k+1^{st}&lt;/script&gt; layer:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/neuralnetexample.png&quot; title=&quot;Neural Net&quot; alt=&quot;Neural Net&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;An example of a neural net with 3 layers and 3 neurons per layer&lt;/h6&gt;

&lt;p&gt;Similar to how neurons are generally organized in layers in the human brain, neurons in neural nets are often organized in layers as well, where neurons on the bottom layer receive signals from the inputs, where neurons in the top layers have their outlets connected to the &quot;answer,&quot; and where are usually no connections between neurons in the same layer (although this is an optional restriction, more complex connectivities require more involved mathematical analysis). We also note that in this example, there are no connections that lead from a neuron in a higher layer to a neuron in a lower layer (i.e., no directed cycles). These neural networks are called &lt;em&gt;feed-forward&lt;/em&gt; neural networks as opposed to their counterparts, which are called &lt;em&gt;recursive&lt;/em&gt; neural networks (again these are much more complicated to analyze and train). For the sake of simplicity, we focus only on feed-forward networks throughout this discussion. Here&#39;s a set of some more important notes to keep in mind:&lt;/p&gt;

&lt;p&gt;1) Although every layer has the same number of neurons in this example, this is not necessary.&lt;/p&gt;

&lt;p&gt;2) It is not required that a neuron has its outlet connected to the inputs of every neuron in the next layer. In fact, selecting which neurons to connect to which other neurons in the next layer is an art that comes from experience. Allowing maximal connectivity will more often than not result in &lt;em&gt;overfitting&lt;/em&gt;, a concept which we will discusss in more depth later.&lt;/p&gt;

&lt;p&gt;3) The inputs and outputs are &lt;em&gt;vectorized&lt;/em&gt; representations. For example, you might imagine a neural network where the inputs are the individual pixel RGB values in an image represented as a vector. The last layer might have 2 neurons which correspond to the answer to our problem: &lt;script type=&quot;math/tex&quot;&gt;[0, 1]&lt;/script&gt; if the image contains a dog, &lt;script type=&quot;math/tex&quot;&gt;[1, 0]&lt;/script&gt; if the image contains a cat, &lt;script type=&quot;math/tex&quot;&gt;[0, 0]&lt;/script&gt; if it contains neither, and &lt;script type=&quot;math/tex&quot;&gt;[1, 1]&lt;/script&gt; if it contains both.&lt;/p&gt;

&lt;p&gt;4) The layers of neurons that lie sandwiched between the first layer of neurons (input layer) and the last layer of neurons (output layer), are called &lt;em&gt;hidden layers&lt;/em&gt;. This is because this is where most of the magic is happening when the neural net tries to solve problems. Taking a look at the activities of hidden layers can tell you a lot about the features the network has learned to extract from the data.&lt;/p&gt;

&lt;h3&gt;Training a Single Neuron&lt;/h3&gt;

&lt;p&gt;Well okay, things are starting to get interesting, but we&#39;re still missing a big chunk of the picture. We know how a neural net can compute answers from inputs, but we&#39;ve been assuming that we know what weights to use to begin with. Finding out what those weights should be is the hard part of the problem, and that&#39;s done through a process called &lt;em&gt;training&lt;/em&gt;. During training, we show the neural net a large number of training examples and iteratively modify the weights to minimize the errors we make on the training examples.&lt;/p&gt;

&lt;p&gt;Let&#39;s start off with a toy example involving a single linear neuron to motivate the process. Every day you grab lunch in the dining hall where your meal consists completely of burgers, fries, and soda. You buy some number of servings of each item. You want to be able to predict how much your meal will cost you, but you don&#39;t know the prices of each individual item. The only thing the cashier will tell you is the total price of the meal.&lt;/p&gt;

&lt;p&gt;How do we solve this problem? Well, we could begin by being smart about picking our training cases, right? For one meal we could buy only a single serving of burgers, for another we could only buy a single serving of fries, and then for our last meal we could buy a single serving of soda. In general, choosing smart training cases is a very good idea. There&#39;s lots of research that shows that by engineering a clever training set, you can make your neural net a lot more effective. The issue with this approach is that in real situations, this rarely ever gets you even 10% of the way to the solution. For example, what&#39;s the analog of this strategy in image recognition?&lt;/p&gt;

&lt;p&gt;Let&#39;s try to motivate a solution that works in general. Take a look at the single neuron we want to train:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/dininghallneuron.png&quot; title=&quot;Dining Hall Neuron&quot; alt=&quot;Dining Hall Neuron&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;The neuron we want to train for the Dining Hall Problem&lt;/h6&gt;

&lt;p&gt;Let&#39;s say we have a bunch of training examples. Then we can calculate what the neural network will output on the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; training example using the simple formula in the diagram. We want to train the neuron so that we pick the optimal weights possible - the weights that minimize the errors we make on the training examples. In this case, let&#39;s say we want to minimize the square error over all of the training examples that we encounter. More formally, if we know that &lt;script type=&quot;math/tex&quot;&gt;t^{(i)}&lt;/script&gt; is the true answer for the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; training example and &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; is the value computed by the neural network, we want to minimize the value of the error function &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E =\frac{1}{2}\sum_{i}(t^{(i)} - y^{(i)})^2&lt;/script&gt;


&lt;p&gt;Now at this point you might be thinking, wait up... Why do we need to bother ourselves with this error function nonsense when we have a bunch of variables (weights) and we have a set of equations (one for each training example)? Couldn&#39;t we just solve this problem by setting up a system of linear system of equations? That would automaically give us an error of zero assuming that we have a consistent set of training examples, right?&lt;/p&gt;

&lt;p&gt;That&#39;s a smart observation, but the insight unfortunately doesn&#39;t generalize well. Remember that although we&#39;re using a linear neuron here, linear neurons aren&#39;t used very much in practice because they&#39;re constrained in what they can learn. And the moment you start using nonlinear neurons like the sigmoidal neurons we talked about, we can no longer set up a system of linear equations!&lt;/p&gt;

&lt;p&gt;So maybe we can use an iterative approach instead that generalizes to nonlinear examples. Let&#39;s try to visualize how we might minimize the squared error over all of the training examples by simplifying the problem. Let&#39;s say we&#39;re dealing with a linear neuron with only two inputs (and thus only two weights, &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;). Then we can imagine a 3-dimensional space where the horizontal dimensions correspond to the weights &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;, and there is one vertical dimension that corresponds to the value of the error function &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;. So in this space, points in the horizontal plane correspond to different settings of the weights, and the height at those points corresponds to the error that we&#39;re incurring, summed over all training cases. If we consider the errors we make over all possible weights, we get a surface in this 3-dimensional space, in particular a quadratic bowl:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/quadraticerror3d.png&quot; title=&quot;Quadratic Error Surface&quot; alt=&quot;Quadratic Error Surface&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;The quadratic error surface for a linear neuron&lt;/h6&gt;

&lt;p&gt;We can also conveniently visualize this surface as a set of elliptical contours, where the minimum error is at the center of the ellipses:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/errorcontours.png&quot; title=&quot;Counter Error Surface&quot; alt=&quot;Contour Error Surface&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Visualizing the error surface as a set of contours&lt;/h6&gt;

&lt;p&gt;So now let&#39;s say we find ourselves somewhere on the horizontal plane (by picking a random initialization for the weights). How would we get ourselves to the point on the horizontal plane with the smallest error value? One strategy is to always move perpendicularly to the contour lines. Take a look, for instance, at the path denoted by the red arrows. Quite clearly, you can see that following this strategy will eventually get us to the point of minimum error.&lt;/p&gt;

&lt;p&gt;What&#39;s particularly interesting is that moving perpendicularly to the contour lines is equivalent to taking the path of steepest descent down the parabolic bowl. This is a pretty amazing result from calculus, and it gives us the name of this general strategy for training neural nets: &lt;em&gt;gradient descent&lt;/em&gt;.&lt;/p&gt;

&lt;h3&gt;Learning Rates and the Delta Rule&lt;/h3&gt;

&lt;p&gt;In practice at each step of moving perpendicular to the contour, we need to determine how far we want to walk before recalculating our new direction. This distance needs to depend on the steepness of the surface. Why? The closer we are to the minimum, the shorter we want to step forward. We know we are close to the minimum, because the surface is a lot flatter, so we can use the steepness as an indicator of how close we are to the minimum. We multiply this measure of steepness with a pre-determined constant factor &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;, the &lt;em&gt;learning rate&lt;/em&gt;. Picking the learning rate is a hard problem. If we pick a learning rate that&#39;s too small, we risk taking too long during the training process. If we pick a learning rate that&#39;s too big, we&#39;ll mostly likely start diverging away from the minimum (this pretty easy to visualize). Modern training algorithms adapt the learning rate to overcome this difficult challenge.&lt;/p&gt;

&lt;p&gt;For those who are interested, putting all the pieces results in what is called the &lt;em&gt;delta rule&lt;/em&gt; for training the linear neuron. The delta rule states that given a learning rate &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;, we ought to change the weight &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt; at each iteration of training by &lt;script type=&quot;math/tex&quot;&gt;\Delta w_k = \sum_i \epsilon x_k(t^{(i)} - y^{(i)})&lt;/script&gt;. Deriving this formula is left as an exercise for the experienced reader. For a hint, study our derivation for a sigmoidal neuron in the next section.&lt;/p&gt;

&lt;p&gt;Unfortunately, just taking the path of steepest descent doesn&#39;t always do the trick when we have nonlinear neurons. The error surface can get complicated and there could be multiple local minimum. As a result, using this procedure could potentially get us to a bad local minimum that isn&#39;t the global minimum. As a result, in practice, training neural nets involves a modification of gradient descent called &lt;em&gt;stochastic gradient descent&lt;/em&gt;, that tries to use randomization and noise to find the global minimum with high probability on a complex error surface.&lt;/p&gt;

&lt;h3&gt;Moving onto the Sigmoidal Neuron *&lt;/h3&gt;

&lt;p&gt;This section and the next will get a little heavy with the math, so just be forewarned. If you&#39;re not comfortable with multivariate calculus, feel free to skip them and move onto the remaining sections. Otherwise, let&#39;s just dive right into it!&lt;/p&gt;

&lt;p&gt;Let&#39;s recall the mechanism by which logistic neurons compute their output value from their inputs:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    z = \sum_k w_kx_k
&lt;/script&gt;


&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    y = \frac{1}{1+e^{-z}}
&lt;/script&gt;


&lt;p&gt;The neuron computes the weighted sum of its inputs, the &lt;em&gt;logit&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. It then feeds &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; into the input function to compute &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, its final output. These functions have very nice derivatives, which makes learning easy! For learning, we want to compute the gradient of the error function with respect to the weights. To do so, we start by taking the derivative of the logit, &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, with respect to the inputs and the weights. By linearity of the logit:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial z}{\partial w_k} = x_k
&lt;/script&gt;


&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial z}{\partial x_k} = w_k
&lt;/script&gt;


&lt;p&gt;Also, quite surprisingly, the derivative of the output with respect to the logit is quite simple if you express it in terms of the output. Verifying this is left as an exercise for the reader:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{dy}{dz} = y(1-y)
&lt;/script&gt;


&lt;p&gt;We then use the chain rule to get the derivative of the output with respect to each weight:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial y}{\partial w_k} = \frac{\partial z}{\partial w_k} \frac{dy}{dz} = x_ky(1-y)
&lt;/script&gt;


&lt;p&gt;Putting all of this together, we can now compute the derivative of the error function with respect to each weight:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial E}{\partial w_k} = \sum_i \frac{\partial y^{(i)}}{\partial w_k} \frac{\partial E}{\partial y^{(i)}} = -\sum_i x_k^{(i)}y^{(i)}\left(1-y^{(i)}\right)\left(t^{(i)} - y^{(i)}\right)
&lt;/script&gt;


&lt;p&gt;Thus, the final rule for modifying the weights becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \Delta w_k = \sum_i \epsilon x_k^{(i)}y^{(i)}\left(1-y^{(i)}\right)\left(t^{(i)} - y^{(i)}\right)
&lt;/script&gt;


&lt;p&gt;As you may notice, the new modification rule is just like the delta rule, except with extra multiplicative terms included to account for the logistic component of the sigmoidal neuron.&lt;/p&gt;

&lt;h3&gt;The Backpropagation Algorithm *&lt;/h3&gt;

&lt;p&gt;Now we&#39;re finally ready to tackle the problem of training multilayer neural networks (instead of just single neurons). So what&#39;s the idea behind backpropagation? We don&#39;t know what the hidden units ought to be doing, but what we can do is compute how fast the error changes as we change a hidden activity. Essentially we&#39;ll be trying to find the path of steepest descent!&lt;/p&gt;

&lt;p&gt;Each hidden unit can affect many output units. Thus, we&#39;ll have to combine many separate effects on the error in an informative way. Our strategy will be one of dynamic programming. Once we have the error derivatives for one layer of hidden units, we&#39;ll use them to compute the error derivatives for the activites of the layer below. And once we find the error derivatives for the activities of the hidden units, it&#39;s quite easy to get the error derivatives for the weights leading into a hidden unit. We&#39;ll redefine some notation for ease of discussion and refer to the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/backprop.png&quot; title=&quot;Backpropagation Diagram&quot; alt=&quot;Backpropagation Diagram&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Reference diagram for the derivation of the backpropagation algorithm&lt;/h6&gt;

&lt;p&gt;The subscript we use will refer to the layer of the neuron. The symbol &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; will refer to the activity of a neuron, as usual. Similarly the symbol &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; will refer to the logit of a neuron. We start by taking a look at the base case of the dynamic programming problem, the error function derivatives at the output layer:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    E = \frac{1}{2} \sum_{j \in output} \left(t_j - y_j\right)^2
    \implies
    \frac{\partial E}{\partial y_j} = -(t_j - y_j)
&lt;/script&gt;


&lt;p&gt;Now we tackle the inductive step. Let&#39;s presume we have the error derivatives for layer &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. We now aim to calculate the error derivatives for the layer below it, layer &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. To do so, we must accumulate information for how the output of a neuron in layer &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; affects the logits of every neuron in layer &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. This can be done as follows, using the fact that the partial derivative of the logit with respect to the incoming output data from the layer beneath is merely the weight of the connection &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial E}{\partial y_i} = \sum_j \frac{dz_j}{dy_i} \frac{\partial E}{\partial z_j} = \sum_j w_{ij} \frac{\partial E}{\partial z_j}
&lt;/script&gt;


&lt;p&gt;Now we can use the following to complete the inductive step:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial E}{\partial z_j} = \frac{dy_j}{dx_j} \frac{\partial E}{\partial y_j} = y_j(1-y_j) \frac{\partial E}{\partial y_j}
&lt;/script&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Combining these two together, we can finally express the partial derivatives of layer &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; in terms of the partial derivatives of layer &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial E}{\partial y_i} = \sum_j w_{ij} y_j (1-y_j) \frac{\partial E}{\partial y_j}
&lt;/script&gt;


&lt;p&gt;Then once we&#39;ve gone through the whole dynamic programming routine, having filled up the table appropriately with all of our partial derivatives (of the error function with respect to the hidden unit activities), we can then determine how the error changes with respect to the weights. This gives us how to modify the weights after each training example:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \frac{\partial E}{\partial w_{ij}} = \frac{\partial z_j}{\partial w_{ij}} \frac{\partial E}{\partial z_j} = y_iy_j(1-y_j)\frac{\partial E}{\partial y_j}
&lt;/script&gt;


&lt;p&gt;In order to do backpropagation with batching of training examples, we merely sum up the partial derivatives over all the training examples in the batch. This gives us the following modification formula:&lt;/p&gt;

&lt;script type=&quot;math/tex;mode=display&quot;&gt;
    \Delta w_{ij} = -\sum_{batch}\epsilon y_iy_j(1-y_j)\frac{\partial E}{\partial y_j}
&lt;/script&gt;


&lt;p&gt;We have succeeded in deriving the backpropagation algorithm for a feed-forward neural net utilizing sigmoidal neurons!&lt;/p&gt;

&lt;h3&gt;The Problem of Overfitting&lt;/h3&gt;

&lt;p&gt;Now let&#39;s say you decide you&#39;re very excited about deep learning and so you want to try to train a neural network of your own to identify objects in an image. You know this is a complicated problem so you use a huge neural network (let&#39;s say 20 layers) and you have 1,000 training examples. You train your neural network using the algorithm we describe, but something&#39;s clearly wrong. Your neural net performs virtually perfectly on your training examples, but when you put it in practice, it performs very poorly! What&#39;s going on here?&lt;/p&gt;

&lt;p&gt;The problem we&#39;ve encountered is called &lt;em&gt;overfitting&lt;/em&gt;, and it happens when you have way too many parameters in your model and not enough training data. To visualize this, let&#39;s consider the figure below, where you want to fit a model to the data points:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/overfitting.png&quot; title=&quot;Overfitting&quot; alt=&quot;Overfitting&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;An example that illustrates the concept of overfitting&lt;/h6&gt;

&lt;p&gt;Which curve would you trust? The line which gets almost no training example exactly? Or the complicated curve that hits every single point in the training set? Most likely you would trust the linear fit instead of the complicated curve because it seems less contrived. This situation is analagous to the neural network we trained. We have way too many parameters, over 100 trillion trillion (or 100 septillion) parameters. It&#39;s no wonder that we&#39;re overfitting!&lt;/p&gt;

&lt;p&gt;So how do we prevent overfitting? The two simplest ways are:&lt;/p&gt;

&lt;p&gt;1) Limit the connectivities of neurons in your model. Architecting a good neural network requires a lot of experience and intuition, and it boils down to giving your model freedom to discover relationships while also constraining it so it doesn&#39;t overfit.&lt;/p&gt;

&lt;p&gt;2) Adding more training examples! Often times you can cleverly add amplify your existing training set (changing illumination, applying shifts and other transformations, etc.).&lt;/p&gt;

&lt;p&gt;There are more sophisticated methods of training that try to directly solve overfitting such as including a dropout layers/neurons, but these methods are beyond the scope of this article&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;We&#39;ve covered a lot of ground, but there&#39;s still a lot more that&#39;s going on in deep learning research. In future articles, I will probably talk more about different kinds of neural architectures (convolutional networks, soft max layers, etc.). I&#39;ll probably also write an article about how to train your own neural network using some of the awesome open source libraries out there, such as Caffe (which allows you to GPU accelerate the training of neural networks). Those of you who are interested in pursuing deep learning further, please get in touch! I love talking about new ideas and projects ❤&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Dec 2014 00:00:00 -0800</pubDate>
        <link>http://nikhilbuduma.com//2014/12/29/deep-learning-in-a-nutshell/</link>
        <guid isPermaLink="true">http://nikhilbuduma.com//2014/12/29/deep-learning-in-a-nutshell/</guid>
      </item>
    
      <item>
        <title>The Cell... Reimagined</title>
        <description>&lt;p&gt;I’ve been obsessed with biology ever since I was a kid. It started all the way back from the time when I had to make regular visits to the cardiologist — one of the side effects of being born with a serious heart defect. My heart defect has (thankfully) since recovered, but those visits had a lasting impression on me. I’d always be interrogating my cardiologist when I walked into his office, pointing to his nifty model of the human heart and asking him question after question. And as I grew up, biology remained to consume a large portion of my active curiosity and imagination.&lt;/p&gt;

&lt;p&gt;But after coming to MIT, I began to become slightly frustrated with with the approach towards traditional biological research. I had worked in biomedical research laboratories since the age of 14 and served as the lab manager for a microbiology research group at San Jose State University, so I had been exposed to a huge volume of research while in high school. Through all my experiences, I found that traditional biologists treated the cell like a black box, resulting in a rather standard operating procedure:&lt;/p&gt;

&lt;p&gt;1) We perturbed the system in certain ways
&lt;br/&gt;2) We measured some output
&lt;br/&gt;3) We made inferences based on the observations&lt;/p&gt;

&lt;p&gt;And while this process has been fruitful over the past few decades, this meta-perspective left me wondering if there was a different way of approaching biological systems.&lt;/p&gt;

&lt;h3&gt;Engineering Biology&lt;/h3&gt;

&lt;p&gt;Evolution has always tackled biological systems in a very unique way. Instead of inference, evolution uses induction. Evolution uses the cell as an engine for natural creation, and this realization prompted me to wonder whether we could take the same approach.&lt;/p&gt;

&lt;p&gt;Just as silicon became the major vehicle of creation and production in the 20th century, I believe that the cell will soon become the major vehicle of creation and production in the 21st. But this shift in approach will entail significant challenges.&lt;/p&gt;

&lt;p&gt;The story of computer systems is one of forward engineering — a human-controlled developmental path. We have the blueprint. Our specifications and standards can be clearly written down and comprehended. We built and control every piece. We understand it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/cell_computer.jpeg&quot; title=&quot;The Cell as a Computer&quot; alt=&quot;The Cell as a Computer&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Image provided by the MIT Synthetic Biology Group&lt;/h6&gt;

&lt;p&gt;Engineering the cell is a completely different cookie. The evolutionary path is barely understood. We are only beginning to understand the blueprint — the specifications and standards are still shrouded in mystery. It’s a beast we have yet to tame.&lt;/p&gt;

&lt;p&gt;But I posit that, maybe, it isn’t necessary to completely understand the cell in order to engineer it. And, maybe, an engineer’s approach to the cell will help us put together pieces of the puzzle we wouldn’t have been able to understand otherwise.&lt;/p&gt;

&lt;h3&gt;Biological Abstractions&lt;/h3&gt;

&lt;p&gt;Borrowing generously from the fields of electrical engineering and computer science, I argue that redesigning biology requires the design of appropriate biological abstractions that simplify the engineering process.&lt;/p&gt;

&lt;p&gt;For those new to the field, I present here a simplified view of a low-level genetic unit — a singular gene and corresponding regulatory elements:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/gene.png&quot; title=&quot;The Genetic Unit&quot; alt=&quot;The Genetic Unit&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;A basic unit, consisting of a promoter, protein-coding region (gene), and termination sequence&lt;/h6&gt;

&lt;p&gt;This unit consists of a protein-coding region, which produces biological activity when transcribed by RNA polymerase. The transcription is regulated by a promoter region, which binds to activators and repressors which control the accessibility of the gene to RNA polymerase. Finally, a terminator immediately follows the protein-coding region to trigger RNA polymerase to complete transcription and fall off of the DNA strand.&lt;/p&gt;

&lt;p&gt;Such building blocks could form the form the foundation of more complex “genetic circuits.” Consider for example, a programming language in which a compiler takes high level descriptions of genetic circuits and turns them into low level instructions For instance, one could imagine a series of two linked units, A and B, where the protein product of unit A binds to the promoter of unit B, thus modulating its expression. The figure below shows how one would implement the high level instruction IPTG → NOT → GREEN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/programming_example.png&quot; title=&quot;Java Lib&quot; alt=&quot;Java Lib&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;A Java library, specialized compiler, and optimization routine that illustrates biological abstraction&lt;/h6&gt;

&lt;p&gt;Each of the symbols IPTG, NOT, and GREEN, correspond to specific pieces that are linked in series as described. The IPTG module consists of two genetic units, one that constitutively expresses the protein LacI (a protein inhibited by the small chemical IPTG). LacI inhibits the transcription of A from the second genetic unit of the module. The result is that in the presence of IPTG, protein A is produced. The NOT module constitutively produces protein B, but it’s expression is inhibited when protein A (the product of the previous module) binds to its promoter region. And lastly, the GREEN module expresses GFP when protein B is bound to its promoter. The result? In the presence of IPTG, protein A is no longer expressed, the level of protein B rises, and consequently, GFP is produced.&lt;/p&gt;

&lt;p&gt;But clearly, this is a little verbose, and just as modern computer language compilers optimize the translation of high level code to low level instructions, an optimization routine would be able to determine that we could remove the NOT module altogether, and exchange the GREEN module’s promoter with the NOT module’s promoter so that protein A inhibits the expression of GFP. Operationally, the result is the same, but the system is stabler, and we have reduced the propagation and contamination delays (extending the EE/CS analogies). Finally, the build process would result in the automated engineering of a cell line housing this construct, enabling the full translation of code to functional genetic circuit.&lt;/p&gt;

&lt;p&gt;By leveraging the inference techniques that have been explored for decades, testing new parts and modules, and evaluating putative circuits through whole cell model computational models (such as the one released by the Covert Lab at Stanford University), we may soon be able to reliably build complex genetic circuits to perform arbitrary computation.&lt;/p&gt;

&lt;h3&gt;Next-Generation Biosensors&lt;/h3&gt;

&lt;p&gt;Probably one of the most interesting applications of a biological programming language is the ability to build custom biosensors. Cells have specialized in sensing and responding to their environments over billions of years, making them particularly versatile at transducing environmental signals. As a result, biosensors have the potential to outperform conventional chemical tests for applications such as pollutant detection, disease diagnosis, etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/biosensor.png&quot; title=&quot;Biosensor&quot; alt=&quot;Biosensor&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Concept probiotic biosensor for monitoring colon health&lt;/h6&gt;

&lt;p&gt;Biosensors traditionally consist of two basic modules: a receptor module for recognizing compounds and a transducer module for reporting the signal. In a future where biological parts are plentiful, this division of labor makes it relatively painless for an engineer to make increasingly complex biosensors. For instance, an engineer could load a single cell line with orthogonal receptor and transducer module pairs, endowing it with specificity for more than one signal.&lt;/p&gt;

&lt;p&gt;One could imagine using the above strategy to engineer yeast to monitor the health of the GI tract. For example, the yeast could express a receptor for a colon cancer biomarker. Upon ligand binding, the yeast upregulates an enzyme that produces a visible, colored pigment. This pigment would then be excreted and observed in the patient’s stool (as a technical side remark, the illustrated example implements trans-activating and cis-repressing riboswitches to prevent steady-state leakage in the absence of biomarker). One could imagine utilizing orthogonality to enable the same yeast to also recognize infections by bacteria, viruses, and multicellular parasites.&lt;/p&gt;

&lt;h3&gt;Attacking Cancer with Viruses&lt;/h3&gt;

&lt;p&gt;Another, rather intriguing, possibility of biological reprogramming is the ability to program cells to die. Apoptosis, biologically programmed cell death, is a hugely important process during normal development and homeostatic maintenance. Being able to induce cells to selectively undergo apoptosis brings up a number of ethical concerns. For instance, a malicious agent could potentially generate super-viruses by loading virus particles with a genetic “kill-switch” circuit.&lt;/p&gt;

&lt;p&gt;But if we are able to overcome the ethical hurdles of reprogramming death, this approach may open up a vastly different method of combatting (and, in a future world where we can genetically modify human zygotes, even preventing) cancer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/cancer.png&quot; title=&quot;Cancer&quot; alt=&quot;Cancer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cancer has a highly altered molecular phenotype compared to the normal somatic cell. Consequently, if we could engineer a “kill-switch” circuit that detects cancerous deviations from the normal molecular phenotype (such as the expression of micro RNAs) and trigger apoptosis when the cell becomes cancerous, we could destroy the tumor before it even has the chance to grow and metastasize. We could load our “kill-switch” into a adenovirus specific to the cancer’s tissue of origin and inject the adenovirus into the patient’s bloodstream.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/cancer_circuit.png&quot; title=&quot;Cancer Kill-Switch&quot; alt=&quot;Cancer Kill-Switch&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;A concept cancer-targeting scheme adapted from Xie et al., 2011&lt;/h6&gt;

&lt;p&gt;After thinking about how one would construct such a circuit and consulting the literature, I put together the scheme depicted in the illustration above. It’s somewhat similar to the module developed by the Weiss group, but their paper, published in Science, takes additional, very necessary steps to prevent leakage of the apoptosis inducer. I omit these components because they distract from the core idea and refer interested readers to the &lt;a target=&quot;_blank&quot; href=&quot;http://www.sciencemag.org/content/333/6047/1307.short&quot;&gt;original article&lt;/a&gt; for the full design.&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;This is really just the tip of the proverbial iceberg in a field so cutting-edge that it’s changing every single day. I would love to keep writing, but it’s nearly 3 AM and I’d probably bore you if I went any longer. But if I haven’t put you to sleep yet and you’d like to talk more, please email me at nkbuduma@gmail.com! I love hearing new ideas ❤&lt;/p&gt;

&lt;div style=&quot;text-align: center;font-family:lato,san serif&quot;&gt;
&lt;br/&gt;
&lt;span style=&#39;font-size:12px&#39;&gt;&lt;i&gt;This article is cross-posted on Medium &lt;a href=&#39;https://medium.com/@nikhilbuduma/the-cell-reimagined-474e53460d6f&#39; target=&#39;_blank&#39;&gt;here&lt;/a&gt;&lt;/i&gt;&lt;/span&gt;
&lt;/div&gt;

</description>
        <pubDate>Sat, 27 Dec 2014 00:00:00 -0800</pubDate>
        <link>http://nikhilbuduma.com//2014/12/27/the-cell-reimagined/</link>
        <guid isPermaLink="true">http://nikhilbuduma.com//2014/12/27/the-cell-reimagined/</guid>
      </item>
    
      <item>
        <title>Hacking the North</title>
        <description>&lt;p&gt;When I first heard about Hack the North at the University of Waterloo, I was immediately on board. Basically free flights to and from Canada, a roof over my head while I&#39;m there, unlimited free food (including poutine), and 36 hours of high energy hacking. It was an irresistable opportunity.&lt;/p&gt;

&lt;h3&gt;The Rejection Letter&lt;/h3&gt;

&lt;p&gt;I decided to apply as a team with two of my close friends from MIT, Anish and Shelby. The three of us were now addicted to the high-octane energy of the hackathon scene. We even had an idea for a project. We were going to build a Google glass application that would take live video feed and create a searchable archive of your life experiences. You&#39;d never have to worry about seeing a flyer in the hallway and then forgetting the phone number or email. Or you could quickly find and extract your friend&#39;s great joke from lunch for your Medium post. It was going to be awesome!&lt;/p&gt;

&lt;p&gt;But then decision emails trickled out, and my excitement slowly faded into dejection:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Hold on a little longer… You’ve been waitlisted! We received an overwhelming 2500+ applicants this year and unfortunately due to our venue&#39;s space limitations, we are unable to offer you an immediate spot at the event...&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Darn. Well that&#39;s a bummer. The three of us were really looking forward to flying up to Canada. And I had never been before either.&lt;/p&gt;

&lt;p&gt;Then something strange happened. Anish pinged the group chat on Facebook, excited about being accepted to the hackathon. Shelby was in too. Confused, I sent an email to the organizing team about the situation, and by the next day, I received a reply. There had been a mix-up due to the volume of people who had applied.&lt;/p&gt;

&lt;p&gt;I was in after all!&lt;/p&gt;

&lt;h3&gt;An Unexpected Journey&lt;/h3&gt;

&lt;p&gt;This section might make this sound like this is going to be something out of a Hobbit movie, but seriously, I would have never guessed that getting to and from a hackathon could ever be as exciting as the hackathon itself.&lt;/p&gt;

&lt;p&gt;My itinerary from Boston to Waterloo seemed like nothing out of the ordinary. A connection through LaGuardia Airport in New York en route to Toronto, followed by a bus organized by Hack the North down to the UWaterloo campus.&lt;/p&gt;

&lt;p&gt;But then things started to get interesting in New York. I located the gate of my connected flight immediately after landing, even though I was starving from not having eaten at all that day, set my stuff down, and got my passport cleared. And then, after doing what any responsible traveler ought to do, I bought a meal at from the restaurant closest to the gate. As I&#39;m finishing up my meal and it&#39;s just about time to board, a United Airways representative announces, &quot;This flight has been cancelled due to mechanical problems.&quot;&lt;/p&gt;

&lt;p&gt;Wait what?&lt;/p&gt;

&lt;p&gt;Immediately people begin to line up to get their flights rescheduled, but I knew that I&#39;d be late if I waited and would most definitely get stranded at the Toronto airport if I didn&#39;t play this smart. I needed to get on the next flight to Toronto.&lt;/p&gt;

&lt;p&gt;I intercepted one of the United representatives and asked him to put me on the next flight that left the airport, whether it was a United flight or some other carrier. He resisted, but after arguing for 5 minutes, he finally gave in. He printed a new ticket and handed it to me. The flight was scheduled to board in 10 minutes, and to depart in 25. It was also in a different terminal, approximately a mile away, so I&#39;d have to go through security again.&lt;/p&gt;

&lt;p&gt;The shuttle wasn&#39;t going to get me there on time, so I grabbed my stuff and bolted. Pretty sure that was the fastest mile I&#39;ve ever run in my life.&lt;/p&gt;

&lt;p&gt;(As a sidenote, the way back was also one hell of a trip. Anish and I got stranded in Philadelphia because our first leg was so delayed that we missed our connection. We spent the night at some substandard hotel, struggling to find something to eat. They didn&#39;t even give us meal vouchers. Never flying United again.)&lt;/p&gt;

&lt;h3&gt;In Canada at Last&lt;/h3&gt;

&lt;p&gt;Right before I boarded my plane, I shot off an email to the Hack the North team letting them know my situation, and asking them to wait for me just in case something went wrong. Thankfully, they got the memo, so someone was waiting for me when I arrived at the airport in Toronto at the wrong terminal. We were both relieved that the bus hadn&#39;t yet arrived before we got to the right spot. And what do you know? There was Shelby!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/timhortons.jpg&quot; title=&quot;Tim Hortons&quot; alt=&quot;Tim Hortons&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Shameless selfies at the airport Tim Hortons&lt;/h6&gt;

&lt;p&gt;Intrigued by the lengendary stories of the coffee shop chain Tim Hortons, we decided to give the place a try. We got a maple flavored donut from the little shop in the terminal, which turned out to be pretty darn good. Finally, after waiting for another hour (I guess traffic&#39;s pretty bad in Canada?), the bus finally arrived!&lt;/p&gt;

&lt;h3&gt;Rounding out the Remembrall Team&lt;/h3&gt;

&lt;p&gt;We all realized that the scope of Remembrall (what we decided to call our project) was probably going to be beyond what the three of us could handle on our own. We&#39;d have to put together a Google glass application, a nontrivial server-side infrastructure for audio and video processing, and a web interface that allowed you to perform some pretty sophisticated search operations to select video clips and compose journal entries. So we found Moaaz from UWaterloo through the Hack the North Facebook group to join our team.&lt;/p&gt;

&lt;p&gt;After the four of us finally found each other on campus and met up with one of my good old friends from the biology Olympiad (we both competed in Singapore in 2012, except she was on Team Canada), we scoped out a hacker space for ourselves on the second floor of the engineering building. We filled it up with sofa chairs that we found scattered across the floor. It even had this great adjoining empty hallway where my teammates could sleep if they were ever tired. I, of course, wouldn&#39;t need it. As they say, sleep is for the weak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sleeping.jpg&quot; title=&quot;Sleeping&quot; alt=&quot;Sleeping&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Some random person, definitely not me, sleeping in the hallway&lt;/h6&gt;

&lt;p&gt;Er... ok fine. You got me there&lt;/p&gt;

&lt;h3&gt;Rekognize the Opportunity and Hustle&lt;/h3&gt;

&lt;p&gt;Over the course of 36 hours, we slowly built our project from the ground up. But one of the roadblocks we hit early on during the weekend seemed like it was going to spell the end for Remembrall. One of the major components of the project was analyzing the video feed streamed from our Google glass and performing image recognition on a per frame basis. We quickly realized that implementing our own recognition system was probably going to be too time consuming, so I spent some time scouring the web for an API we could leverage. It wasn&#39;t too long before I found Rekognition, which gave us exactly what we were looking for. Excitedly, I showed this to my team.&lt;/p&gt;

&lt;p&gt;But then the harsh reality of economics hit me. If we were going to do real time video processing, even at a measly rate of a single frame per second, we&#39;d have to make over 3,000 API calls just to analyze an hour of video. That was (at the time) beyond what we could get out of the free tier of the service, so we needed another plan of action.&lt;/p&gt;

&lt;p&gt;Out of desperation, I decided to try the &quot;Chat with us!&quot; feature on the Rekognition website. I don&#39;t know what I was thinking. It was 3 AM, and no sane person would expect a reply from a startup&#39;s customer service at such a late hour. And even if they did see the message, there was a slim chance we&#39;d get what we wanted.&lt;/p&gt;

&lt;p&gt;But by some miracle, someone did respond, and they also upgraded my account to the pro service free of charge. I chucked a Hail Mary, and the unthinkable happened. I guess the moral of the story is to never be afraid to pick up the darn phone, because you never know what will happen unless you try.&lt;/p&gt;

&lt;h3&gt;Round One Begins&lt;/h3&gt;

&lt;p&gt;By the end of 36 hours we were in good shape. After wrangling with the Google Glass SDK and (ab)using the Wit.ai API, Anish was able to get the Google Glass application and audio to text components working. I recorded a significant amount of video footage, integrated the image/concept recognition components, and got elastic search to work on the back end. And finally, Shelby and Moaaz were able to get the web interface to work beautifully.&lt;/p&gt;

&lt;p&gt;When were greeted by our judging panel for round one, we were ready to give the beast a test run. And it went amazingly. So amazingly that we were interrupted halfway through the presentation by one of the judges, who exclaimed that Remembrall was one of the most impressive hackathon demos she had seen. Fist bumps flew through the air as we walked out of the first round.&lt;/p&gt;

&lt;p&gt;This is why we do what we do. To build things that bring magic to life.&lt;/p&gt;

&lt;h3&gt;The Closing&lt;/h3&gt;

&lt;p&gt;We ended up being selected as one of the top 10 teams of the hackathon and presented our demo on stage! If you&#39;d like to watch our presentation, you can find it right here! We also have a &lt;a target=&quot;_blank&quot; href=&quot;http://remembrall.me&quot;&gt;website&lt;/a&gt; for the project, which is still a work in progress.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;br/&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/XHwZ8Hd22YQ&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br/&gt;
&lt;/div&gt;


&lt;p&gt;We also ended up winning the Thiel Fellowship sponsorship prize, which meant an Oculus Rift DK2 for each of us in addition to a fully paid trip to Las Vegas for the Thiel Summit!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/oculus.jpg&quot; title=&quot;Oculus&quot; alt=&quot;Oculus&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;New Oculus Rift DK2&#39;s!&lt;/h6&gt;

&lt;p&gt;The weekend was over, but we left in high spirits. Thank you for throwing this hackathon, Hack the North. I know when I look back on my college experience later in life, this is one of the things I&#39;ll fondly remember.&lt;/p&gt;

&lt;div style=&quot;text-align: center;font-family:lato,san serif&quot;&gt;
&lt;br/&gt;
&lt;span style=&#39;font-size:12px&#39;&gt;&lt;i&gt;Thanks to &lt;a href=&#39;https://medium.com/@davefontenot&#39; target=&#39;_blank&#39;&gt;Dave Fontenot&lt;/a&gt; for encouraging me to finish this!&lt;/i&gt;&lt;/span&gt;
&lt;/div&gt;



</description>
        <pubDate>Mon, 22 Sep 2014 00:00:00 -0700</pubDate>
        <link>http://nikhilbuduma.com//2014/09/22/Hacking-the-North/</link>
        <guid isPermaLink="true">http://nikhilbuduma.com//2014/09/22/Hacking-the-North/</guid>
      </item>
    
      <item>
        <title>Greylock Hackfest @ Medium!</title>
        <description>&lt;p&gt;As of writing this post I’ve gone approximately 36 hours without sleeping after attending my first serious hackathon, Greylock Hackfest @ Medium. As you can probably guess, I’m pretty exhausted, but I want to put everything down in writing before I forget it. I hope you’ll forgive me if I veer off topic.
By the way, I also find it kind of amusing that I’m making my first post on Medium after attending the hackathon they hosted this weekend.&lt;/p&gt;

&lt;h3&gt;The Idea — HyperInk&lt;/h3&gt;

&lt;p&gt;Late Thursday night, we got together to brainstorm for the hackathon. Thinking of any idea was actually pretty difficult. We knew we wanted to create something more than just your typical web application. We wanted to build something cool, something that would connect the physical world with the digital one. A rather ambitious undertaking for a 23 hour project, but as my teammate Kevin Kwok (@antimatter) would say, #YOLO.&lt;/p&gt;

&lt;h3&gt;Let the Hacking Begin&lt;/h3&gt;

&lt;p&gt;On the day of the hackathon, I made my way to Anish’s place at around 8:30 in the morning so Shelby could pick us up. Kevin was already in San Francisco, having partied with some Dropbox guys the night before, so we agreed to meet up at Medium HQ around 9:30 — a whole hour before the doors even officially opened. I guess we were pretty excited.&lt;/p&gt;

&lt;p&gt;As expected, we were told that we were way too early when we showed up on the premises, but Julie (the awesome person from Greylock who was basically running the whole show) told us that we could get first dibs on claiming space for our team, so we were pretty excited. With an iPhone stand built from K’Nex in hand, we all crossed the street to wait at a crowded San Francisco coffee shop while passersby stared curiously at our contraption.&lt;/p&gt;

&lt;p&gt;At 10:15 we promptly returned to Medium to scout out the hackerspace. We found a sweet room with a table, a couch, and a huge television screen that we staked out for ourselves, and began to chill, chat, and meet other teams while we waited for the hackathon to officially start.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/postimg01.jpeg&quot; title=&quot;Early Version&quot; alt=&quot;Early Version&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;An early version of our iOS application detects pieces of papers, highlights the edges in red, and the vertices in green&lt;/h6&gt;

&lt;p&gt;At noon, the organizers officially declared that we could begin working, and we took off immediately. Anish and I began chugging away at the computer vision pieces of the project, building a hybrid C++/Objective C iOS application. Using some pretty sophisticated tools from the OpenCV library, we were able to detect rectangles representing pieces of papers and reverse the perspective warping to generate readable output (for those of you technical people, we ended up using a bilateral filter, Canny edge detection, Graham scan convex hull finding, and polygonal contour approximations). Meanwhile, Shelby built out a front-end using AngularJS (Kevin and I prefer React, but we tried our best to help troubleshoot during those rare instances late into the night/morning when Shelby couldn’t figure something out), and Kevin built out our server in Node in addition to implementing some client-side computer vision in pure JavaScipt (specifically, the stroke width transform).&lt;/p&gt;

&lt;p&gt;During the hack session, Kevin never hesitated to entertain the team, even if it meant sacrificing an hour or two to build a silly web app displaying a chicken dancing to the song “Heads will Roll” by the Yeah Yeah Yeahs ft. A-Trak. That song is now stuck in my head. Thanks Kevin.&lt;/p&gt;

&lt;p&gt;To the objective reader, the above paragraphs might make it seem like everything during the hackathon went smoothly. And while it’s true that we may have had it better than some other teams (who were frantically searching for Arduinos and Android wear at 2 AM or had to deal with their Kinect killing their Windows device drivers), our conference room was still pretty close to the definition of chaos. K’Nex pieces, scraps of test paper, and food wrappers (thank you Greylock for all of the amazing food!) lay all over the place. Our whiteboard was completely covered in random diagrams as I tried to bash out some perspective geometry. We had to switch from a cURL interface to a web socket system when we realized that our iOS application wasn’t communicating with our server fast enough and was leaking memory. And at 9 AM, just 2 hours before the deadline, we had to email the nice folk at Firebase to ask for help with integrating a persistence layer into our system for implementing real-time document annotations. Thankfully they came on over to help us with it in person!&lt;/p&gt;

&lt;p&gt;Finally at noon today, we had a finished product that worked really well. We demoed it to a couple of people, and they were pretty impressed with what we had put together. We had the chance to practice our presentation a couple of times, and I think it’s pretty accurate to say that we felt like we were on top of our game.&lt;/p&gt;

&lt;h3&gt;The Real Roller Coaster&lt;/h3&gt;

&lt;p&gt;Multiple computers, iPhone, and K’Nex setup in hand, the four of us walked into the presentation room ready to go for the first round. As I introduced ourselves, and began to describe our hack, Anish, Kevin, and Shelby began to everything ready for the demo.&lt;/p&gt;

&lt;p&gt;Except it didn’t work.&lt;/p&gt;

&lt;p&gt;Immediately the four of us went into panic mode under the hood and watched in despair as our computers failed to stably connect to the WiFi. Without a working demo, we did the best to describe what we built. Anish showed them the iPhone application, which could detect the paper on a table but was only able to send a single frame to a server. Finally one of the judges interrupted and told us that we were out of the time. The judges asked us a couple of questions, and we answered them, but we were completely demoralized. At the end of our train wreck of a demo, we walked out with our heads hanging low.&lt;/p&gt;

&lt;p&gt;When we got back to our conference room, Anish sat down really quietly and sadly stared down at his iPhone. Kevin wandered around the hackerspace for a bit and then left for his parents’ hotel. Shelby was clearly disappointed but still tried her best to cheer us all up. I couldn’t really see what I looked like, but I’m certain I looked terrible because I sure felt like it.&lt;/p&gt;

&lt;p&gt;We were certain that we didn’t make it to the next stage of competition after that debacle. We had built something we were all really proud of, but at that moment, it felt like the collective 92 hours of figurative blood, sweat, and tears had all gone to waste.&lt;/p&gt;

&lt;p&gt;At around 3:30 PM, the participants all gathered together to watch as the top 10 hacks would be called up to the stage one by one to demo their projects. Despite our low spirits, Anish and I decided to watch. At the very least, we’d learn a thing or two from the seasoned veterans that we may be able to put to use at our next hackathon. After listening to a pitch for a video sharing app that randomizes the background song and a demo of a twist on IFTTT that was based on hardware components instead of third party software APIs, Anish and I decided that we were too exhausted to keep listening, so we began to walk back to our conference room and rest.&lt;/p&gt;

&lt;p&gt;But as we began to turn into the hallway, we heard something so unexpected that we both instinctually turned our heads.&lt;/p&gt;

&lt;p&gt;“All right! Come on up CopyCat! And after that we will Hyper_Ink come on up and demo their project — congratulations on being selected as one of the top 10 hacks of the Hackfest!”&lt;/p&gt;

&lt;p&gt;Oh. My. God.&lt;/p&gt;

&lt;h3&gt;The Time is Now&lt;/h3&gt;

&lt;p&gt;Anish and I couldn’t believe our ears. Immediately, we ran back to our conference room and woke up a sleeping Shelby (she had a terrible migraine after staying up all night). We called up Kevin and told him to get back to Medium as fast as we could.&lt;/p&gt;

&lt;p&gt;We made the top 10 with a nonexistent demo. It was just unreal.&lt;/p&gt;

&lt;p&gt;I quickly ran over to Julie and asked if our presentation could be delayed until later so that we could wait for Kevin to come back. And as soon as he did, we decided to record ourselves performing a demo in our room just in case the WiFi hated us again. I&#39;ve linked the video here, but there&#39;s no sound:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;br/&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/Q5a451II2R4&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;br/&gt;
&lt;/div&gt;


&lt;p&gt;When our turn came up, the four of us walked up in front of over 150 people, quickly set up, and knocked it out the park. The videos got our point across with little extra effort, the live demo worked pretty darn well, and none of us forgot any of our talking points. We even had some extra time to talk about the impacts of the technology in education as a dirt-cheap replacement of the iPad as well as our future vision of sharing whole physical whiteboards in addition to just pieces of paper.&lt;/p&gt;

&lt;p&gt;When we finished, we were greeted with cheers, high-fives, and congratulations. The four of us felt like we were on top of the world.&lt;/p&gt;

&lt;h3&gt;Going Home&lt;/h3&gt;

&lt;p&gt;Although we didn’t win one of the top three prizes, making it to the finals was amazing — especially given that it was only our first serious hackathon. Smiles on our faces, Shelby, Anish, and I drove back in merry spirits.&lt;/p&gt;

&lt;p&gt;We made many new friends, had an unforgettable time, and built something that we were really proud of, all at the same time. I could go on and on talking about how awesome the hackathon was and how grateful I am for the team who put it together, but I think the weekend is best summed up by something I said to Shelby and Anish on the way back home:&lt;/p&gt;

&lt;p&gt;“Wow! This was one of the best summers of the weekend!”&lt;/p&gt;

&lt;p&gt;I need some sleep.&lt;/p&gt;

&lt;div style=&quot;text-align: center;font-family:lato,san serif&quot;&gt;
&lt;br/&gt;
&lt;span style=&#39;font-size:12px&#39;&gt;&lt;i&gt;This article is cross-posted on Medium &lt;a href=&#39;https://medium.com/hackathon-adventures/greylock-hackfest-medium-e9b6cc2e82a4&#39; target=&#39;_blank&#39;&gt;here&lt;/a&gt;&lt;/i&gt;&lt;/span&gt;
&lt;/div&gt;

</description>
        <pubDate>Mon, 21 Jul 2014 00:00:00 -0700</pubDate>
        <link>http://nikhilbuduma.com//2014/07/21/greylock-hackfest-medium/</link>
        <guid isPermaLink="true">http://nikhilbuduma.com//2014/07/21/greylock-hackfest-medium/</guid>
      </item>
    
      <item>
        <title>CEE Capitol Hill Luncheon Remarks</title>
        <description>&lt;p&gt;Probably one of the most formative experiences of my time in high school was participating in the USA Biology Olympiad, a contest sponsored by the Center for Excellence and Education. For those who may not be familiar with the format of the competition, the Olympiad is a series of examinations that culminates in a 2-week National Finals camp held at Purdue University where, the U.S. team for the international competition is decided. Every single year, approximately 10,000 students participate nationwide, and only 4 are lucky enough to move onto the International Biology Olympiad.&lt;/p&gt;

&lt;p&gt;I participated in the Biology Olympiad during all four years of my high school career. My sophomore year, I qualified to the National Finals. And during my my junior and senior years, I had the honor of attending the international Olympiad in Singapore (2012) and Switzerland (2013). After working with the program for over 5 years, I can say without hesitation that being able to interact with world class mentors and some of the smartest students in the world has been an amazing experience.&lt;/p&gt;

&lt;p&gt;As a result, when I was asked to speak in Washington, D.C., about the program and it&#39;s impact on high school education, I was ecstatic. There were over a hundred people attendance - donors, CEE staff, and politicians (including Rep. Mike Honda!) - and I was humbled to be presenting alongside people as impressive as Jud Bowman, the CEO of Appia, and Dr. Nancy Hsia Akerman from the Stratospheric Protection Division of the EPA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/Honda_Buduma_Chaskin.jpg&quot; title=&quot;Picture with Rep. Mike Honda and Mel Chaskin&quot; alt=&quot;Picture with Rep. Mike Honda and Mel Chaskin&quot; /&gt;&lt;/p&gt;

&lt;h6&gt;Hanging out with Rep. Mike Honda and Mel Chaskin, CEO of Vangaurd Research&lt;/h6&gt;

&lt;p&gt;If you&#39;re interested in the content of the luncheon, the CEE published a newsletter about the event &lt;a href=&#39;http://www.cee.org/sites/default/files/newsletters/cee_fall_2014.pdf&#39; target=&#39;_blank&#39;&gt;here&lt;/a&gt;. I wanted to take this opportunity to share my remarks at the luncheon:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Almost precisely 3 years ago, I opened up an email that would completely change my life. That email was an invitation to participate in the 2011 USA Biology Olympiad National Finals, a 2 week long camp where the top 20 high school students in the nation come together to explore the life sciences on an unbelievably deep level and compete for a spot on the 4-person Team USA at the International Olympiad. Now, if someone had told me back then that this would be the single most important email I would open during my high school career, my younger self probably would have dismissed them in disbelief. But in retrospect, being invited to the National Finals for the first time my sophomore year opened so many doors that it’s impossible for me to imagine what the past three years of my life would’ve been like otherwise.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I think one of the most valuable aspects of the biology Olympiad is the community built around it. Learning is never a purely individual pursuit, it’s highly cooperative, dynamic, and inter-disciplinary. At the national level, the Olympiad brings together people who are deeply passionate about biology, but at the same time come from vastly different cultural and academic backgrounds.  Surrounded by a team of world class peers, researchers, and mentors, I’ve learned more over the cumulative 6 weeks I spent at the National Finals than I learned in 4 years of school.  I can confidently say that the rigor of the Olympiad is unmatched by any other high school program in the country. And the fact that the U.S. team has brought home 4 gold medals not one, not two, but three years in a row is a testament to that fact. But, in addition to just helping me build a strong knowledge base, the program has also taught me to ask insightful questions and empowered me with the toolset necessary to design my own experiments and discover the answers. In fact, using everything I’ve learned through the Olympiad, I’ve been able to conduct independent research projects on topics ranging from low cost screening techniques for pharmaceutical products to improving the composition of the whooping cough vaccine.  And the beneficial effects of the Olympiad don’t just stop at the highest level of competition. Through my personal experiences, I’ve found that the community spirit trickles all the way down to the school level. After bringing the biology Olympiad to my high school, more and more people interested in biology began to study together, and as a result, the science program at my high school has strengthened significantly.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;On an even broader level, participating at the International level gave me an eye-opening glimpse of the world beyond my immediate community. I will never forget learning some basic Arabic from a boy representing the United Arab Emirates, discussing the issues of segregation and racial discrimination with the team from South Africa, and hearing the life story of a budding biologist from Iran, who had kept her love of science a secret from her family in fear that they would not accept her. I will never forget the nights we spent in the common area, passing around a Tupperware container of exotic plant specimen most of us had never seen before. And of course, I will never forget standing in front a huge crowd to announce my teammate&#39;s birthday the night after the practical exam and hearing the room burst into a surprisingly harmonious ensemble of the song &quot;Happy Birthday&quot; sung simultaneously in at least twenty different languages.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You see, for me, and for tens of thousands of students all over the nation, the U.S.A Biology Olympiad has been a life-changing experience. It’s programs like these that inspire students to take their education to the next level, tackle some of the society’s most important problems, and engage the world in ways as citizens of a global community. None of this would be possible without the Center for Excellence and Education, who have organized such an amazing program, Ms. Kathy Frame and Dr. Clark Gedney, who have made the USA Biology Olympiad the spectacular success it is today, and all of you whose support is crucial to the success of the Olympiad and its ability to foster the education of future generations of students.  Thank you.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For anybody in high school looking to learn more biology, I&#39;d highly recommend participating in the USA Biology Olympiad. It&#39;s had a hugely positive impact on my life and the lives of all my friends who have gone through the program. I hope the donors in the audience got the message and continue to support the USA Biology Olympiad. I know Rep. Mike Honda gets it ;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/fistbump.jpg&quot; title=&quot;Fist Bump&quot; alt=&quot;Fist Bump&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 01 May 2014 00:00:00 -0700</pubDate>
        <link>http://nikhilbuduma.com//2014/05/01/speaking-at-the-cee-congressional-luncheon/</link>
        <guid isPermaLink="true">http://nikhilbuduma.com//2014/05/01/speaking-at-the-cee-congressional-luncheon/</guid>
      </item>
    
  </channel>
</rss>
